{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "import keras\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop word removal\n",
    "from nltk.corpus import stopwords\n",
    "stop={'και','ή', 'γιατί', 'as', 'until ','με','από','από','από','από','από','από','από', 'πριν','μετά','πάνω','κάτω','προς','από','πάνω','κάτω','in','out','on',\"πάλι\",\"πάλι\",\"έπειτα\",\"μία\",\"εδώ\",\"εκεί\",\"πότε\",\"πού\",'κάθε', 'λίγοι', 'περισσότεροι', 'περισσότεροι', 'άλλοι', 'ορισμένοι', 'μόνο', 'ίδιοι', 'ίδιοι', 'έτσι', 'από ',' πολύ ',' πολύ ',' θα ',' ακριβώς ',' θα πρέπει ','τώρα' } #add related stop word here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv('Greek/offenseval-greek-training-v1.csv',sep='\\t')\n",
    "df_test = pd.read_csv('test/testset_taska.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2707</td>\n",
       "      <td>@USER Θέλω να των δω από εδώ και εμπρός αν δεν...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2251</td>\n",
       "      <td>#survivorgr Α Και 60 φορές και με διαφορετικού...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9814</td>\n",
       "      <td>Και μου έλεγε η γυναίκα μου το πρωί πάρε την τ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8949</td>\n",
       "      <td>κατω τα χερια απο τον #κυρανακης</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6913</td>\n",
       "      <td>@USER μην μας το παιζεις πονοψυχη,κρυφορατσιστ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              tweet\n",
       "0  2707  @USER Θέλω να των δω από εδώ και εμπρός αν δεν...\n",
       "1  2251  #survivorgr Α Και 60 φορές και με διαφορετικού...\n",
       "2  9814  Και μου έλεγε η γυναίκα μου το πρωί πάρε την τ...\n",
       "3  8949                   κατω τα χερια απο τον #κυρανακης\n",
       "4  6913  @USER μην μας το παιζεις πονοψυχη,κρυφορατσιστ..."
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8743\n",
      "8743\n",
      "1544\n",
      "1544\n"
     ]
    }
   ],
   "source": [
    "#marge all data from files that has been read\n",
    "twee=[]\n",
    "ids=[]\n",
    "for i in range(len(df_test)):\n",
    "    ids.append(df_test['id'][i])\n",
    "    twee.append(df_test['tweet'][i])\n",
    "\n",
    "\n",
    "#marge all data from files that has been read\n",
    "q_a=[]\n",
    "label=[]\n",
    "for i in range(len(df_csv)):\n",
    "    label.append(df_csv['subtask_a'][i])\n",
    "    q_a.append(df_csv['tweet'][i])\n",
    "\n",
    "print(len(q_a))\n",
    "print(len(label))\n",
    "print(len(twee))\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1544\n",
      "1544\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty space from the begining of a sentance\n",
    "TWEETS=[]\n",
    "for i in twee:\n",
    "    sen1=i.replace('@USER','')\n",
    "    sen1=sen1.replace('@user','')\n",
    "    sen1=sen1.replace(\"#\", ' ')\n",
    "\n",
    "    sen1=\" \".join(sen1.split())\n",
    "    TWEETS.append(sen1)\n",
    "twee=TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8743"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert labeling to 0 and 1\n",
    "lb=[]\n",
    "for i in label:\n",
    "    if i=='OFF':\n",
    "        lb.append(1)\n",
    "    if i=='NOT':\n",
    "        lb.append(0)\n",
    "labels=lb\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8743"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove empty space from the begining of a sentance\n",
    "QA=[]\n",
    "for i in q_a:\n",
    "    sen=i.replace('@USER','')\n",
    "    sen=sen.replace('@user','')\n",
    "    sen=sen.replace(\"#\", ' ')\n",
    "    sen=\" \".join(sen.split())\n",
    "    QA.append(sen)\n",
    "# print(QA)\n",
    "q_a=QA\n",
    "len(q_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read data files\n",
    "comments_attack=pd.DataFrame()\n",
    "comments_attack['comment']=q_a\n",
    "comments_attack['label']=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = comments_attack\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code will remove stop word\n",
    "for c in stop: #stop word removing\n",
    "    dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(' '+c+' ', ' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "8738    0\n",
      "8739    0\n",
      "8740    0\n",
      "8741    0\n",
      "8742    0\n",
      "Name: label, Length: 8743, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataframe['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_attack['label'][990]==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       @USER Οι μουσουλμάνες που τις βιάζουν έτσι κ α...\n",
      "1       Η Κάτια προσπαθεί να πείσει οτι δεν είναι ελέφ...\n",
      "2       Καλά γιατί λες ότι, είσαι νέος αφού γεννήθηκες...\n",
      "3               Με Φατσεα ξεκινησαμε...... #Kokkinopotami\n",
      "4                       #gntmgr Κάτια πόσο γλυκιά, εμετός\n",
      "                              ...                        \n",
      "8738    Σοβαρό δημοκρατικό έλλειμμα η απουσία γυναικών...\n",
      "8739                     Γιουροβίζιον βλέπουμε #XFactorGR\n",
      "8740    Στην Κάτια να δοκιμάσουμε κάτι τέτοιο παρακαλώ...\n",
      "8741    Γιατι δεν ειναι για μοντέλο! Γι’αυτο κερα Βίκυ...\n",
      "8742    Κανονικά ο τύπος θα έπρεπε να ντρέπεται μόνο κ...\n",
      "Name: comment, Length: 8743, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dataframe['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in dataframe['comment']:\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and validation sets\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(dataframe['comment'], dataframe['label'], test_size=0.04, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train comments length:  8393\n",
      "test comments length:  350\n"
     ]
    }
   ],
   "source": [
    "print('train comments length: ',len(train_x))\n",
    "print('test comments length: ',len(valid_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(dataframe['comment'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# # ngram level tf-idf \n",
    "# tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(3,4), max_features=5000)\n",
    "# tfidf_vect_ngram.fit(dataframe['comment'])\n",
    "# xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "# xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# # characters level tf-idf\n",
    "# tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "# tfidf_vect_ngram_chars.fit(dataframe['comment'])\n",
    "# xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "# xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('data/wiki-news-300d-1M.vec', encoding=\"utf8\")):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(dataframe['comment'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure e\n",
    "# qual length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, xtrain, ytrain, xvalid, yvalid): \n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(xtrain, ytrain)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(xvalid)\n",
    "    accuracy = metrics.accuracy_score(predictions, yvalid)\n",
    "    f1score = metrics.f1_score(yvalid, predictions, average='weighted')\n",
    "    return accuracy, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Naive Bayes on Word Level TF IDF Vectors\n",
    "# accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
    "# print(\"NB, WordLevel TF-IDF:   accuracy: %s     f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# # Naive Bayes on Ngram Level TF IDF Vectors\n",
    "# accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)\n",
    "# print(\"NB, N-Gram Vectors:   accuracy: %s     f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# # Naive Bayes on Character Level TF IDF Vectors\n",
    "# accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)\n",
    "# print(\"NB, CharLevel Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, WordLevel TF-IDF:   accuracy: 0.8571428571428571   f1 score: 0.8430848124488849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjahan18\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
    "print(\"LR, WordLevel TF-IDF:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# # Linear Classifier on Ngram Level TF IDF Vectors\n",
    "# accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)\n",
    "# print(\"LR, N-Gram Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "# accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)\n",
    "# print(\"LR, CharLevel Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjahan18\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vect',tfidf_vect_ngram_chars),\n",
    "    ('clf', linear_model.LogisticRegression()),\n",
    "])\n",
    "\n",
    "clf = clf.fit(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1544\n",
      "1544\n"
     ]
    }
   ],
   "source": [
    "Labels=[]\n",
    "IDE=[]\n",
    "for i in range(len(twee)):\n",
    "    IDE.append(ids[i])\n",
    "    predictions = clf.predict([twee[i]])\n",
    "    if predictions[0]==1:\n",
    "        Labels.append('OFF')\n",
    "    elif predictions[0]==0:\n",
    "        Labels.append('NOT')\n",
    "print(len(Labels))\n",
    "print(len(IDE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = { 'q_a' : IDE,'label' : Labels}\n",
    "df = pd.DataFrame(my_dict)\n",
    "df.to_csv('submission',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to one_hot\n",
    "train_y_onehot = keras.utils.to_categorical(train_y, 3)\n",
    "valid_y_onehot = keras.utils.to_categorical(valid_y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(xtrain, ytrain, xvalid, yvalid, epochs = 3):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(3, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "    model.fit(xtrain, ytrain,\n",
    "              batch_size=256,\n",
    "              epochs=epochs)\n",
    "    predictions = model.predict(xvalid)\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    accuracy = model.evaluate(xvalid, yvalid, verbose=0)\n",
    "    f1score = metrics.f1_score(valid_y, predictions, average='weighted')\n",
    "    return accuracy, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy, f1score = cnn(train_seq_x, train_y_onehot, valid_seq_x, valid_y_onehot)\n",
    "# print(\"CNN, Word Embeddings acuuracy accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(xtrain, ytrain, xvalid, yvalid, epochs = 1):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer1 = layers.LSTM(128)(embedding_layer)\n",
    "    dropout1 = layers.Dropout(0.5)(lstm_layer1)\n",
    "    #lstm_layer2 = layers.LSTM(128)(dropout1)\n",
    "    #dropout2 = layers.Dropout(0.5)(lstm_layer2)\n",
    "    # Add the output Layers\n",
    "    output_layer = layers.Dense(3, activation=\"softmax\")(dropout1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(xtrain, ytrain,\n",
    "              batch_size=256,\n",
    "              epochs=3)\n",
    "    \n",
    "    predictions = model.predict(xvalid)\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    accuracy = model.evaluate(xvalid, yvalid, verbose=0)\n",
    "    f1score = metrics.f1_score(valid_y, predictions, average='weighted')\n",
    "    return accuracy, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy, f1score = lstm(train_seq_x, train_y_onehot, valid_seq_x, valid_y_onehot)\n",
    "# print(\"LSTM, Word Embeddings accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
