{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "import keras\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'should', 'by', 'or', 'down', 'out', 'a', 'against', 'below', 'to', 'few', 'than', 'for', 'once', 'because', 'about', 'some', 'so', 'same', 'and', 'how', 'further', 'until', 'other', 'the', 'more', 'there', 'with', 'too', 'from', 'while', 'off', 'then', 'again', 'here', 'now', 'of', 'an', 'through', 'under', 'as', 'during', 'both', 'most', 'just', 'where', 'each', 'why', 'on', 'over', 'between', 'when', 'after', 'in', 'own', 'will', 'at', 'all', 'into', 'any', 'only', 'if', 'very', 'up', 'before', 'above'}\n"
     ]
    }
   ],
   "source": [
    "#stop word removal\n",
    "from nltk.corpus import stopwords\n",
    "stop={'a', 'of', 'on', 'the','a','an','the','and','if','or','because','as','until','while','of','at','by','for','with','about','against','between','into','through','during','before','after','above','below','to','from','up','down','in','out','on','off','over','under','again','further','then','once','here','there','when','where','why','how','all','any','both','each','few','more','most','other','some','only','own','same','so','than','too','very','will','just','should','now' } #add related stop word here\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>average</th>\n",
       "      <th>std</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1159524764874149888</td>\n",
       "      <td>Tune in to a fresh new episode of @USER radio,...</td>\n",
       "      <td>0.094849</td>\n",
       "      <td>0.147089</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1159535775714881539</td>\n",
       "      <td>@USER As we age, our perception of ourself may...</td>\n",
       "      <td>0.097000</td>\n",
       "      <td>0.158873</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>861553587373174785</td>\n",
       "      <td>Rest, drink from my book of tea, rejuvenate, a...</td>\n",
       "      <td>0.093193</td>\n",
       "      <td>0.107156</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1159536446736412672</td>\n",
       "      <td>Love is so beautiful and I love the idea of it.</td>\n",
       "      <td>0.079121</td>\n",
       "      <td>0.150167</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1159536476432011264</td>\n",
       "      <td>What’s the best hotel in Vegas? I need a spot ...</td>\n",
       "      <td>0.088381</td>\n",
       "      <td>0.153040</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0  1159524764874149888  Tune in to a fresh new episode of @USER radio,...   \n",
       "1  1159535775714881539  @USER As we age, our perception of ourself may...   \n",
       "2   861553587373174785  Rest, drink from my book of tea, rejuvenate, a...   \n",
       "3  1159536446736412672    Love is so beautiful and I love the idea of it.   \n",
       "4  1159536476432011264  What’s the best hotel in Vegas? I need a spot ...   \n",
       "\n",
       "    average       std label  \n",
       "0  0.094849  0.147089   NOT  \n",
       "1  0.097000  0.158873   NOT  \n",
       "2  0.093193  0.107156   NOT  \n",
       "3  0.079121  0.150167   NOT  \n",
       "4  0.088381  0.153040   NOT  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_csv3 = pd.read_csv('2019 olid/olid-training-v1.0.tsv',sep='\\t')\n",
    "df_csv4 = pd.read_csv('task_a_distant_NOT.csv',sep=',')\n",
    "df_csv5 = pd.read_csv('task_a_distant_OFF.csv',sep=',')\n",
    "df_test = pd.read_csv('English/test_a_tweets.tsv',sep='\\t')\n",
    "df_csv4.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#marge all data from files that has been read\n",
    "twee=[]\n",
    "ids=[]\n",
    "for i in range(len(df_test)):\n",
    "    ids.append(df_test['id'][i])\n",
    "    twee.append(df_test['tweet'][i])\n",
    "    \n",
    "av=[]\n",
    "st=[]\n",
    "\n",
    "#marge all data from files that has been read\n",
    "\n",
    "q_a=[]\n",
    "label=[]\n",
    "# for i in range(len(df_csv3)):\n",
    "#     label.append(df_csv3['subtask_a'][i])\n",
    "#     q_a.append(df_csv3['tweet'][i])\n",
    "    \n",
    "for i in range(len(df_csv4)):\n",
    "    label.append(df_csv4['label'][i])\n",
    "    q_a.append(df_csv4['text'][i])\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(df_csv5)):\n",
    "    label.append(df_csv5['label'][i])\n",
    "    q_a.append(df_csv5['text'][i])\n",
    "    av.append(df_csv5['average'][i])\n",
    "    st.append(df_csv5['std'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16619\n",
      "16619\n",
      "3887\n",
      "3887\n",
      "0.9567039679079568\n",
      "0.9000045757524653\n",
      "0.16502006652768367\n",
      "0.03709859575808765\n"
     ]
    }
   ],
   "source": [
    "print(len(q_a))\n",
    "print(len(label))\n",
    "print(len(twee))\n",
    "print(len(ids))\n",
    "print(max(av))\n",
    "print(min(av))\n",
    "print(max(st))\n",
    "print(min(st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty space from the begining of a sentance\n",
    "TWEETS=[]\n",
    "for i in twee:\n",
    "    sen1=i.replace('@USER','')\n",
    "    sen1=sen1.replace('@user','')\n",
    "    sen1=sen1.replace(\"#\", ' ')\n",
    "    sen1=sen1.replace('@USER','')\n",
    "    sen1=sen1.replace(\"NEWLINE_TOKEN\", \" \")\n",
    "    sen1=sen1.replace(\"@\", \"\")\n",
    "    sen1=sen1.replace(\"TAB_TOKEN\", \" \")\n",
    "    sen1=sen1.replace(\" w \", ' with ')\n",
    "    sen1=sen1.replace(\" mf \", ' motherfucker ')\n",
    "    sen1=sen1.replace(\" IDGAF \", ' i dont give a fuck ')\n",
    "    sen1=sen1.replace(\" IDRC \", ' I Dont Really Care ')\n",
    "    sen1=sen1.replace(\" tf \", ' the fuck ')\n",
    "    sen1=sen1.replace(\" wtf \", ' what the fuck ')\n",
    "    sen1=sen1.replace(\" yanno \", ' you know ')\n",
    "    sen1=sen1.replace(\" igt \", ' i know right ')\n",
    "    sen1=sen1.replace(\" r \", ' are ')\n",
    "    sen1=sen1.replace(\" tbh \", ' to be honest ')\n",
    "    sen1=sen1.replace(\" ik \", ' i know ')\n",
    "    sen1=sen1.replace(\" af \", ' as fuck ')\n",
    "    sen1=sen1.replace(\" des \", ' this ')\n",
    "    sen1=sen1.replace(\" bout \", ' about ')\n",
    "    sen1=sen1.replace(\" em \", ' them ')\n",
    "    sen1=sen1.replace(\" stg \", ' swear to god ')\n",
    "    sen1=sen1.replace(\" bj \", ' blow job ')\n",
    "    sen1=sen1.replace(\" ig \", ' i guess ')\n",
    "    sen1=sen1.replace(\" fagg \", ' faggot ')\n",
    "    sen1=sen1.replace(\" fag \", ' faggot ')\n",
    "    sen1=sen1.replace(\" tfw \", ' That Feel When ')\n",
    "    sen1=sen1.replace(\" wa \", ' was ')\n",
    "    sen1=sen1.replace(\" n \", ' and ')\n",
    "    sen1=sen1.replace(\" y0 \", ' you ')\n",
    "    sen1=sen1.replace(\" dat \", ' that ')\n",
    "    sen1=sen1.replace(\" YO \", ' you ')\n",
    "    sen1=\" \".join(sen1.split())\n",
    "    TWEETS.append(sen1)\n",
    "twee=TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert labeling to 0 and 1\n",
    "lb=[]\n",
    "for i in label:\n",
    "    if i=='OFF':\n",
    "        lb.append(1)\n",
    "    if i=='NOT':\n",
    "        lb.append(0)\n",
    "labels=lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty space from the begining of a sentance\n",
    "QA=[]\n",
    "for i in q_a:\n",
    "    sen=i.replace('@USER','')\n",
    "    sen=sen.replace('@user','')\n",
    "    sen=sen.replace(\"#\", ' ')\n",
    "    sen=\" \".join(sen.split())\n",
    "    QA.append(sen)\n",
    "\n",
    "q_a=QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #this code will stem text\n",
    "stem_both_q_a=[]\n",
    "for i in q_a:\n",
    "    ps = PorterStemmer() #stemming word removing\n",
    "    words = str(i).split(' ') \n",
    "    s=''\n",
    "    for w in words: \n",
    "        stem=ps.stem(w)\n",
    "        s=s+' '+stem\n",
    "    s=s.strip()\n",
    "\n",
    "    stem_both_q_a.append(s)\n",
    "q_a=stem_both_q_a\n",
    "\n",
    "# print(q_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read data files\n",
    "comments_attack=pd.DataFrame()\n",
    "comments_attack['comment']=q_a\n",
    "comments_attack['label']=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = comments_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "# dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace('@USER',''))\n",
    "# dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "# dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "# dataframe['comment'] = dataframe['comment'].apply(lambda x: x.lower())\n",
    "# dataframe['comment'] = dataframe['comment'].apply((lambda x: re.sub('[/(){}\\[\\]\\|@,;]','',x)))\n",
    "# dataframe['comment'] = dataframe['comment'].apply((lambda x: re.sub('[^0-9a-z #+_]','',x)))\n",
    "# dataframe['comment'] = dataframe['comment'].apply((lambda x: re.sub(' +',' ',x)))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" w \", ' with '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" mf \", ' motherfucker '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" IDGAF \", ' i dont give a fuck '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" IDRC \", ' I Dont Really Care '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" tf \", ' the fuck '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" wtf \", ' what the fuck '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" yanno \", ' you know '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" igt \", ' i know right '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" r \", ' are '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" tbh \", ' to be honest '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" ik \", ' i know '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" af \", ' as fuck '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" des \", ' this '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" bout \", ' about '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" em \", ' them '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" stg \", ' swear to god '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" bj \", ' blow job '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" ig \", ' i guess '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" fagg \", ' faggot '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" fag \", ' faggot '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" tfw \", ' That Feel When '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" wa \", ' was '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" n \", ' and '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" y0 \", ' you '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" dat \", ' that '))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\" YO \", ' you '))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code will remove stop word\n",
    "for c in stop: #stop word removing\n",
    "    dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(' '+c+' ', ' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        1\n",
      "1        1\n",
      "2        0\n",
      "3        1\n",
      "4        0\n",
      "        ..\n",
      "29854    1\n",
      "29855    1\n",
      "29856    1\n",
      "29857    1\n",
      "29858    1\n",
      "Name: label, Length: 29859, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataframe['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_attack['label'][990]==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        Tune in to a fresh new episode of radio, with ...\n",
      "1        As we age, our perception of ourself may chang...\n",
      "2        Rest, drink from my book of tea, rejuvenate, a...\n",
      "3          Love is so beautiful and I love the idea of it.\n",
      "4        What’s the best hotel in Vegas? I need a spot ...\n",
      "                               ...                        \n",
      "16614                                        Piece of shit\n",
      "16615    this afro latino shit is so annoying shut the ...\n",
      "16616                          fucking disgrace to favelas\n",
      "16617    Treat a bitch like her credit score poor as fuck!\n",
      "16618    Fucking getting robbed MARTIN Atkinson ignored...\n",
      "Name: comment, Length: 16619, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dataframe['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in dataframe['comment']:\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and validation sets\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(dataframe['comment'], dataframe['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train comments length:  23887\n",
      "test comments length:  5972\n"
     ]
    }
   ],
   "source": [
    "print('train comments length: ',len(train_x))\n",
    "print('test comments length: ',len(valid_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "# tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "# tfidf_vect.fit(dataframe['comment'])\n",
    "# xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "# xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# # ngram level tf-idf \n",
    "# tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(3,4), max_features=5000)\n",
    "# tfidf_vect_ngram.fit(dataframe['comment'])\n",
    "# xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "# xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(dataframe['comment'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('data/wiki-news-300d-1M.vec', encoding=\"utf8\")):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(dataframe['comment'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure e\n",
    "# qual length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, xtrain, ytrain, xvalid, yvalid): \n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(xtrain, ytrain)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(xvalid)     \n",
    "    accuracy = metrics.accuracy_score(predictions, yvalid)\n",
    "    f1score = metrics.f1_score(yvalid, predictions, average='weighted')\n",
    "    return accuracy, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, WordLevel TF-IDF:   accuracy: 0.8921634293369055     f1 score: 0.8894525844114596\n",
      "NB, N-Gram Vectors:   accuracy: 0.775954454119223     f1 score: 0.753545347531771\n",
      "NB, CharLevel Vectors:   accuracy: 0.8747488278633624   f1 score: 0.869119125958184\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
    "print(\"NB, WordLevel TF-IDF:   accuracy: %s     f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)\n",
    "print(\"NB, N-Gram Vectors:   accuracy: %s     f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)\n",
    "print(\"NB, CharLevel Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjahan18\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, CharLevel Vectors:   accuracy: 0.8908238446081714   f1 score: 0.8876314213794875\n"
     ]
    }
   ],
   "source": [
    "# # Linear Classifier on Word Level TF IDF Vectors\n",
    "# accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
    "# print(\"LR, WordLevel TF-IDF:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# # Linear Classifier on Ngram Level TF IDF Vectors\n",
    "# accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)\n",
    "# print(\"LR, N-Gram Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)\n",
    "print(\"LR, CharLevel Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to one_hot\n",
    "train_y_onehot = keras.utils.to_categorical(train_y, 3)\n",
    "valid_y_onehot = keras.utils.to_categorical(valid_y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjahan18\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Count Vectors:   accuracy: 0.8007367716008037   f1 score: 0.7835138049126037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjahan18\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\mjahan18\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\mjahan18\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, WordLevel TF-IDF:   accuracy: 0.6764902880107166   f1 score: 0.5459490138959839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjahan18\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\mjahan18\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors TF-IDF:   accuracy: 0.6764902880107166   f1 score: 0.5459490138959839\n",
      "SVM, CharLevel Vectors:   accuracy: 0.6764902880107166   f1 score: 0.5459490138959839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjahan18\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# SVM Classifier on Count Vectors\n",
    "accuracy, f1score = train_model(svm.SVC(), xtrain_count, train_y, xvalid_count,valid_y)\n",
    "print(\"SVM, Count Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# SVM Classifier on Word Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(svm.SVC(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
    "print(\"SVM, WordLevel TF-IDF:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)\n",
    "print(\"SVM, N-Gram Vectors TF-IDF:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# SVM Classifier on Character Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(svm.SVC(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)\n",
    "print(\"SVM, CharLevel Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(xtrain, ytrain, xvalid, yvalid, epochs = 3):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(3, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "    model.fit(xtrain, ytrain,\n",
    "              batch_size=256,\n",
    "              epochs=epochs)\n",
    "    predictions = model.predict(xvalid)\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    accuracy = model.evaluate(xvalid, yvalid, verbose=0)\n",
    "    f1score = metrics.f1_score(valid_y, predictions, average='weighted')\n",
    "    return accuracy, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "23887/23887 [==============================] - 16s 671us/step - loss: 0.4059 - accuracy: 0.8317\n",
      "Epoch 2/3\n",
      "23887/23887 [==============================] - 16s 661us/step - loss: 0.2530 - accuracy: 0.8966\n",
      "Epoch 3/3\n",
      "23887/23887 [==============================] - 16s 658us/step - loss: 0.2328 - accuracy: 0.9033\n",
      "CNN, Word Embeddings acuuracy accuracy:0.9022102952003479     f1 score: 0.9016268453975631\n"
     ]
    }
   ],
   "source": [
    "accuracy, f1score = cnn(train_seq_x, train_y_onehot, valid_seq_x, valid_y_onehot)\n",
    "print(\"CNN, Word Embeddings acuuracy accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(xtrain, ytrain, xvalid, yvalid, epochs = 1):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer1 = layers.LSTM(128)(embedding_layer)\n",
    "    dropout1 = layers.Dropout(0.5)(lstm_layer1)\n",
    "    #lstm_layer2 = layers.LSTM(128)(dropout1)\n",
    "    #dropout2 = layers.Dropout(0.5)(lstm_layer2)\n",
    "    # Add the output Layers\n",
    "    output_layer = layers.Dense(3, activation=\"softmax\")(dropout1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(xtrain, ytrain,\n",
    "              batch_size=256,\n",
    "              epochs=3)\n",
    "    \n",
    "    predictions = model.predict(xvalid)\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    accuracy = model.evaluate(xvalid, yvalid, verbose=0)\n",
    "    f1score = metrics.f1_score(valid_y, predictions, average='weighted')\n",
    "    return accuracy, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "23887/23887 [==============================] - 70s 3ms/step - loss: 0.3936 - accuracy: 0.8414\n",
      "Epoch 2/3\n",
      "23887/23887 [==============================] - 77s 3ms/step - loss: 0.2667 - accuracy: 0.8849\n",
      "Epoch 3/3\n",
      "23887/23887 [==============================] - 74s 3ms/step - loss: 0.2469 - accuracy: 0.8912\n",
      "LSTM, Word Embeddings accuracy:0.8995311260223389     f1 score: 0.8962878061266843\n"
     ]
    }
   ],
   "source": [
    "accuracy, f1score = lstm(train_seq_x, train_y_onehot, valid_seq_x, valid_y_onehot)\n",
    "print(\"LSTM, Word Embeddings accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjahan18\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7903549899531145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjahan18\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7836570663094441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjahan18\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7314132618888145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjahan18\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8749162759544541\n"
     ]
    }
   ],
   "source": [
    "# RF on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print(accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print(accuracy)\n",
    "\n",
    "# RF on Word Level TF IDF Vectors ngram\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(accuracy)\n",
    "\n",
    "# RF on Word Levelon Character Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjahan18\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vect',tfidf_vect_ngram_chars),\n",
    "    ('clf', linear_model.LogisticRegression()),\n",
    "])\n",
    "\n",
    "clf = clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3887\n",
      "3887\n"
     ]
    }
   ],
   "source": [
    "Labels=[]\n",
    "IDE=[]\n",
    "for i in range(len(twee)):\n",
    "    IDE.append(ids[i])\n",
    "    predictions = clf.predict([twee[i]])\n",
    "    if predictions[0]==1:\n",
    "        Labels.append('OFF')\n",
    "    elif predictions[0]==0:\n",
    "        Labels.append('NOT')\n",
    "print(len(Labels))\n",
    "print(len(IDE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = { 'q_a' : IDE,'label' : Labels}\n",
    "df = pd.DataFrame(my_dict)\n",
    "df.to_csv('English_submission2',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
