{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "import keras\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #stop word removal\n",
    "from nltk.corpus import stopwords\n",
    "stop={'και','ή', 'γιατί', 'as', 'until ','με','από','από','από','από','από','από','από', 'πριν','μετά','πάνω','κάτω','προς','από','πάνω','κάτω','in','out','on',\"πάλι\",\"πάλι\",\"έπειτα\",\"μία\",\"εδώ\",\"εκεί\",\"πότε\",\"πού\",'κάθε', 'λίγοι', 'περισσότεροι', 'περισσότεροι', 'άλλοι', 'ορισμένοι', 'μόνο', 'ίδιοι', 'ίδιοι', 'έτσι', 'από ',' πολύ ',' πολύ ',' θα ',' ακριβώς ',' θα πρέπει ','τώρα' } #add related stop word here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv('Danish/offenseval-da-training-v1.tsv',sep='\\t')\n",
    "df_test = pd.read_csv('Danish/offenseval-da-test-v1-nolabels.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3131</td>\n",
       "      <td>Jeg tror det vil være dejlig køligt, men jeg v...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>711</td>\n",
       "      <td>Så kommer de nok til at investere i en ny cyke...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2500</td>\n",
       "      <td>Nu er det jo også de Ikea-aber der har lavet s...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2678</td>\n",
       "      <td>128 Varme emails, er vi enige om at det er sex...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>784</td>\n",
       "      <td>Desværre tyder det på, at amerikanerne er helt...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2955</td>\n",
       "      <td>170</td>\n",
       "      <td>Har sgu lidt en anelse om... det her kunne mås...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2956</td>\n",
       "      <td>1226</td>\n",
       "      <td>Ind og ruske tremmer med hende,Den syge kælling!!</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2957</td>\n",
       "      <td>2596</td>\n",
       "      <td>fedtmule</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2958</td>\n",
       "      <td>1802</td>\n",
       "      <td>##HAR I HØRT DET?</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2959</td>\n",
       "      <td>2809</td>\n",
       "      <td>Kommer det bag på nogen? Det er jo fucking var...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2960 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet subtask_a\n",
       "0     3131  Jeg tror det vil være dejlig køligt, men jeg v...       NOT\n",
       "1      711  Så kommer de nok til at investere i en ny cyke...       NOT\n",
       "2     2500  Nu er det jo også de Ikea-aber der har lavet s...       OFF\n",
       "3     2678  128 Varme emails, er vi enige om at det er sex...       NOT\n",
       "4      784  Desværre tyder det på, at amerikanerne er helt...       NOT\n",
       "...    ...                                                ...       ...\n",
       "2955   170  Har sgu lidt en anelse om... det her kunne mås...       NOT\n",
       "2956  1226  Ind og ruske tremmer med hende,Den syge kælling!!       OFF\n",
       "2957  2596                                           fedtmule       NOT\n",
       "2958  1802                                  ##HAR I HØRT DET?       NOT\n",
       "2959  2809  Kommer det bag på nogen? Det er jo fucking var...       OFF\n",
       "\n",
       "[2960 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv.head(5556)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1382</td>\n",
       "      <td>Der er syriske \"flygtninge\" som rejser til Ira...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1384</td>\n",
       "      <td>Danmark = Vitryssland?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>547</td>\n",
       "      <td>Ja tvangsfjernelser af børn på urigtige oplysn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1269</td>\n",
       "      <td>Han kan ikke Svensk og forventer et job. Hvis ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1695</td>\n",
       "      <td>NED MED SVENSKEN!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>1591</td>\n",
       "      <td>Tilpas hurtig brille; den er godkendt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1769</td>\n",
       "      <td>Min nye skrivebordsbaggrund.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>2443</td>\n",
       "      <td>Desværre er Luxembourg oppe på 4.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>3221</td>\n",
       "      <td>Lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>400</td>\n",
       "      <td>lol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>329 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                              tweet\n",
       "0    1382  Der er syriske \"flygtninge\" som rejser til Ira...\n",
       "1    1384                             Danmark = Vitryssland?\n",
       "2     547  Ja tvangsfjernelser af børn på urigtige oplysn...\n",
       "3    1269  Han kan ikke Svensk og forventer et job. Hvis ...\n",
       "4    1695                                  NED MED SVENSKEN!\n",
       "..    ...                                                ...\n",
       "324  1591             Tilpas hurtig brille; den er godkendt.\n",
       "325  1769                       Min nye skrivebordsbaggrund.\n",
       "326  2443                  Desværre er Luxembourg oppe på 4.\n",
       "327  3221                                                Lol\n",
       "328   400                                                lol\n",
       "\n",
       "[329 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(5556)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2960\n",
      "2960\n",
      "329\n",
      "329\n"
     ]
    }
   ],
   "source": [
    "#marge all data from files that has been read\n",
    "twee=[]\n",
    "ids=[]\n",
    "for i in range(len(df_test)):\n",
    "    ids.append(df_test['id'][i])\n",
    "    twee.append(df_test['tweet'][i])\n",
    "\n",
    "\n",
    "#marge all data from files that has been read\n",
    "q_a=[]\n",
    "label=[]\n",
    "for i in range(len(df_csv)):\n",
    "    label.append(df_csv['subtask_a'][i])\n",
    "    q_a.append(df_csv['tweet'][i])\n",
    "\n",
    "print(len(q_a))\n",
    "print(len(label))\n",
    "print(len(twee))\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1544\n",
      "1544\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty space from the begining of a sentance\n",
    "TWEETS=[]\n",
    "for i in twee:\n",
    "    sen1=i.replace('@USER','')\n",
    "    sen1=sen1.replace('@user','')\n",
    "    sen1=sen1.replace(\"#\", ' ')\n",
    "    sen1=\" \".join(sen1.split())\n",
    "    TWEETS.append(sen1)\n",
    "twee=TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2960"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert labeling to 0 and 1\n",
    "lb=[]\n",
    "for i in label:\n",
    "    if i=='OFF':\n",
    "        lb.append(1)\n",
    "    if i=='NOT':\n",
    "        lb.append(0)\n",
    "labels=lb\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2960"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove empty space from the begining of a sentance\n",
    "QA=[]\n",
    "for i in q_a:\n",
    "    sen=str(i)\n",
    "    sen=sen.replace('@USER','')\n",
    "    sen=sen.replace('@user','')\n",
    "    sen=\" \".join(sen.split())\n",
    "    QA.append(sen)\n",
    "# print(QA)\n",
    "q_a=QA\n",
    "len(q_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read data files\n",
    "comments_attack=pd.DataFrame()\n",
    "comments_attack['comment']=q_a\n",
    "comments_attack['label']=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = comments_attack\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code will remove stop word\n",
    "for c in stop: #stop word removing\n",
    "    dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(' '+c+' ', ' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       0\n",
      "2       1\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "2955    0\n",
      "2956    1\n",
      "2957    0\n",
      "2958    0\n",
      "2959    1\n",
      "Name: label, Length: 2960, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataframe['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_attack['label'][990]==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Jeg tror det vil være dejlig køligt, men jeg v...\n",
      "1       Så kommer de nok til at investere i en ny cyke...\n",
      "2       Nu er det jo også de Ikea-aber der har lavet s...\n",
      "3       128 Varme emails, er vi enige om at det er sex...\n",
      "4       Desværre tyder det på, at amerikanerne er helt...\n",
      "                              ...                        \n",
      "2955    Har sgu lidt en anelse om... det her kunne mås...\n",
      "2956    Ind og ruske tremmer med hende,Den syge kælling!!\n",
      "2957                                             fedtmule\n",
      "2958                                    ##HAR I HØRT DET?\n",
      "2959    Kommer det bag på nogen? Det er jo fucking var...\n",
      "Name: comment, Length: 2960, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dataframe['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in dataframe['comment']:\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and validation sets\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(dataframe['comment'], dataframe['label'], test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train comments length:  2812\n",
      "test comments length:  148\n"
     ]
    }
   ],
   "source": [
    "print('train comments length: ',len(train_x))\n",
    "print('test comments length: ',len(valid_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(dataframe['comment'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# # ngram level tf-idf \n",
    "# tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(3,4), max_features=5000)\n",
    "# tfidf_vect_ngram.fit(dataframe['comment'])\n",
    "# xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "# xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(dataframe['comment'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('data/wiki-news-300d-1M.vec', encoding=\"utf8\")):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(dataframe['comment'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure e\n",
    "# qual length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, xtrain, ytrain, xvalid, yvalid): \n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(xtrain, ytrain)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(xvalid)\n",
    "    accuracy = metrics.accuracy_score(predictions, yvalid)\n",
    "    f1score = metrics.f1_score(yvalid, predictions, average='weighted')\n",
    "    return accuracy, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Naive Bayes on Word Level TF IDF Vectors\n",
    "# accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
    "# print(\"NB, WordLevel TF-IDF:   accuracy: %s     f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# # Naive Bayes on Ngram Level TF IDF Vectors\n",
    "# accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)\n",
    "# print(\"NB, N-Gram Vectors:   accuracy: %s     f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# # Naive Bayes on Character Level TF IDF Vectors\n",
    "# accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)\n",
    "# print(\"NB, CharLevel Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, WordLevel TF-IDF:   accuracy: 0.9256756756756757   f1 score: 0.8899478425794216\n",
      "LR, CharLevel Vectors:   accuracy: 0.9256756756756757   f1 score: 0.8899478425794216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjahan18\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\mjahan18\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
    "print(\"LR, WordLevel TF-IDF:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# # Linear Classifier on Ngram Level TF IDF Vectors\n",
    "# accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)\n",
    "# print(\"LR, N-Gram Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)\n",
    "print(\"LR, CharLevel Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjahan18\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vect',tfidf_vect_ngram_chars),\n",
    "    ('clf', linear_model.LogisticRegression()),\n",
    "])\n",
    "\n",
    "clf = clf.fit(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329\n",
      "329\n"
     ]
    }
   ],
   "source": [
    "Labels=[]\n",
    "IDE=[]\n",
    "for i in range(len(twee)):\n",
    "    IDE.append(ids[i])\n",
    "    predictions = clf.predict([twee[i]])\n",
    "    if predictions[0]==1:\n",
    "        Labels.append('OFF')\n",
    "    elif predictions[0]==0:\n",
    "        Labels.append('NOT')\n",
    "print(len(Labels))\n",
    "print(len(IDE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = { 'q_a' : IDE,'label' : Labels}\n",
    "df = pd.DataFrame(my_dict)\n",
    "df.to_csv('danish_submission_2',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to one_hot\n",
    "train_y_onehot = keras.utils.to_categorical(train_y, 3)\n",
    "valid_y_onehot = keras.utils.to_categorical(valid_y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(xtrain, ytrain, xvalid, yvalid, epochs = 3):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(3, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "    model.fit(xtrain, ytrain,\n",
    "              batch_size=256,\n",
    "              epochs=epochs)\n",
    "    predictions = model.predict(xvalid)\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    accuracy = model.evaluate(xvalid, yvalid, verbose=0)\n",
    "    f1score = metrics.f1_score(valid_y, predictions, average='weighted')\n",
    "    return accuracy, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy, f1score = cnn(train_seq_x, train_y_onehot, valid_seq_x, valid_y_onehot)\n",
    "# print(\"CNN, Word Embeddings acuuracy accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(xtrain, ytrain, xvalid, yvalid, epochs = 1):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer1 = layers.LSTM(128)(embedding_layer)\n",
    "    dropout1 = layers.Dropout(0.5)(lstm_layer1)\n",
    "    #lstm_layer2 = layers.LSTM(128)(dropout1)\n",
    "    #dropout2 = layers.Dropout(0.5)(lstm_layer2)\n",
    "    # Add the output Layers\n",
    "    output_layer = layers.Dense(3, activation=\"softmax\")(dropout1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(xtrain, ytrain,\n",
    "              batch_size=256,\n",
    "              epochs=3)\n",
    "    \n",
    "    predictions = model.predict(xvalid)\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    accuracy = model.evaluate(xvalid, yvalid, verbose=0)\n",
    "    f1score = metrics.f1_score(valid_y, predictions, average='weighted')\n",
    "    return accuracy, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy, f1score = lstm(train_seq_x, train_y_onehot, valid_seq_x, valid_y_onehot)\n",
    "# print(\"LSTM, Word Embeddings accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
